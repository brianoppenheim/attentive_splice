{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bert_splice.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l9WNc45vfxLd",
        "outputId": "0caa1604-af8a-4d6a-968a-0487a72519bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7k6OAhXSYNY6",
        "outputId": "963171ae-73b8-45d7-b0c6-84cc18c11f39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 688
        }
      },
      "source": [
        "!pip3 install torch\n",
        "!pip3 install transformers\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.17.4)\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/70/1a/364556102943cacde1ee00fdcae3b1615b39e52649eddbf54953e5b144c9/transformers-2.2.1-py3-none-any.whl (364kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 4.9MB/s \n",
            "\u001b[?25hCollecting regex\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/8e/cbf2295643d7265e7883326fb4654e643bfc93b3a8a8274d8010a39d8804/regex-2019.11.1-cp36-cp36m-manylinux1_x86_64.whl (643kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 65.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1f/8e/ed5364a06a9ba720fddd9820155cc57300d28f5f43a6fd7b7e817177e642/sacremoses-0.0.35.tar.gz (859kB)\n",
            "\u001b[K     |████████████████████████████████| 860kB 56.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.4)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.10.27)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/3d/efb655a670b98f62ec32d66954e1109f403db4d937c50d779a75b9763a29/sentencepiece-0.1.83-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 56.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.0)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.2.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.27 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.13.27)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.9.11)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.27->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.27->boto3->transformers) (2.6.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.35-cp36-none-any.whl size=883999 sha256=8ad6e93c6a07ef321259d64da3c11eaeaae5a2e6b343246c07cb76113c00af77\n",
            "  Stored in directory: /root/.cache/pip/wheels/63/2a/db/63e2909042c634ef551d0d9ac825b2b0b32dede4a6d87ddc94\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: regex, sacremoses, sentencepiece, transformers\n",
            "Successfully installed regex-2019.11.1 sacremoses-0.0.35 sentencepiece-0.1.83 transformers-2.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ihioMLoR30M3",
        "outputId": "b914bfd9-8f68-4c05-9bb6-f0ef4dd2ee81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        }
      },
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertModel, BertForTokenClassification, BertConfig\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import sys\n",
        "from torch.optim import Adam\n",
        "from torch.nn import CrossEntropyLoss\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from itertools import combinations_with_replacement, permutations\n",
        "import datetime\n",
        "from torch.nn.functional import softmax\n",
        "import itertools"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZgLSUcu9toXh",
        "outputId": "e389d9aa-a5a9-40d6-f0d5-5876be0edf5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "n_gpu = torch.cuda.device_count()\n",
        "torch.cuda.get_device_name(0)\n",
        "cuda0 = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(cuda0)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "g3_Isxq9BPQt",
        "outputId": "8354b9c4-f10d-4293-cfed-45639725e13c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gputil\n",
            "  Downloading https://files.pythonhosted.org/packages/ed/0e/5c61eedde9f6c87713e89d794f01e378cfd9565847d4576fa627d758c554/GPUtil-1.4.0.tar.gz\n",
            "Building wheels for collected packages: gputil\n",
            "  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gputil: filename=GPUtil-1.4.0-cp36-none-any.whl size=7410 sha256=8e825ce1a4a4e251257f2295786a6781dcdfa071ec48e83cb8cbdbb74583d421\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/77/07/80562de4bb0786e5ea186911a2c831fdd0018bda69beab71fd\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.4.0\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.6/dist-packages (0.5.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KaMoUAxLoNbO",
        "outputId": "2dcaf341-c3e7-4c53-e31f-085c7daf1b96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isn’t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gen RAM Free: 12.6 GB  | Proc size: 502.5 MB\n",
            "GPU RAM Free: 16270MB | Used: 10MB | Util   0% | Total 16280MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdY_hWJ5WmeJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import precision_recall_curve\n",
        "import matplotlib.pyplot as plt\n",
        "from inspect import signature\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import auc\n",
        "\n",
        "def plot_AUCPRC(labels, predictions):\n",
        "\t\"\"\"labels: a 1d list of all the labels (0 or 1) for our samples\n",
        "\t\tpredictions: a 1d list of all the probabilities (between 0 and 1)\n",
        "\t\t             for each one of our samples. It should be the probability that\n",
        "\t\t             that this sample belongs to the second class (e.g a splice site.)\n",
        "\t\"\"\"\n",
        "\t# In matplotlib < 1.5, plt.fill_between does not have a 'step' argument\\\n",
        "\tprecision, recall, _ = precision_recall_curve(labels, predictions)\n",
        "\tauc = auc(recall, precision)\n",
        "\tstep_kwargs = ({'step': 'post'}\n",
        "\t               if 'step' in signature(plt.fill_between).parameters\n",
        "\t               else {})\n",
        "\tplt.step(recall, precision, color='b', alpha=0.2,\n",
        "\t         where='post')\n",
        "\tplt.fill_between(recall, precision, alpha=0.2, color='b', **step_kwargs)\n",
        "\n",
        "\tplt.xlabel('Recall')\n",
        "\tplt.ylabel('Precision')\n",
        "\tplt.ylim([0.0, 1.05])\n",
        "\tplt.xlim([0.0, 1.0])\n",
        "\tplt.title('2-class Precision-Recall curve: auc={0:0.2f}'.format(\n",
        "\t          auc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "piLtetopn2dX",
        "colab": {}
      },
      "source": [
        "def process_data(path, kmer_type):\n",
        "    first = True\n",
        "    file_table = []\n",
        "    with open(path) as fp:\n",
        "        i = 0\n",
        "        for line in fp:\n",
        "            if not first:\n",
        "                new_entry = []\n",
        "                split_line = line.split()\n",
        "                new_entry.append(split_line[0])\n",
        "                new_entry.append(split_line[1])\n",
        "                sep = ' '\n",
        "                kmers = list(map(lambda x: x.replace(']', '').replace('[', '').replace(',','').replace('\\'', ''), split_line[2:1002]))\n",
        "                if '--PAD--' in kmers:\n",
        "                    kmers = kmers[:kmers.index('--PAD--')]\n",
        "                kmers_string = sep.join(kmers)\n",
        "                new_entry.append(kmers_string)\n",
        "                new_entry.append(list(map(lambda x: int(x.replace(']', '').replace('[', '').replace(',','').replace('.0','').replace('\\'','')), split_line[1002:])))\n",
        "                file_table.append(new_entry)\n",
        "                i += 1\n",
        "            first = False\n",
        "    \n",
        "    return handle_tokenization(file_table, kmer_type)\n",
        "\n",
        "def handle_tokenization(file_table, kmer_type):\n",
        "    MAX_LEN = 1002\n",
        "    fourmers_path = '/content/drive/My Drive/k_mers/fourmers.txt'\n",
        "    hexamers_path = '/content/drive/My Drive/k_mers/hexamers.txt'\n",
        "    octamers_path = '/content/drive/My Drive/k_mers/octamers.txt'\n",
        "    kmers_path = fourmers_path\n",
        "    if kmer_type == 'hexamers':\n",
        "      kmers_path = hexamers_path\n",
        "    elif kmer_type == 'octamers':\n",
        "      kmers_path = octamers_path\n",
        "    formatted_hexamers = ['[CLS] ' + f[2] + ' [SEP]' for f in file_table]\n",
        "    labels = [[0] + f[3] + [0] for f in file_table]\n",
        "    tokenizer = BertTokenizer(kmers_path, max_len=MAX_LEN)\n",
        "    attention_masks = [np.concatenate([np.ones((len(a.split()))), np.zeros((MAX_LEN - len(a.split())))]) for a in formatted_hexamers]\n",
        "    \n",
        "    return tokenizer, formatted_hexamers, attention_masks, labels\n",
        "\n",
        "\n",
        "\n",
        "def gradient_descent(l):\n",
        "    optimizer.zero_grad()\n",
        "    l.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "def write_data(batch, batch_size, labels_array, total_loss, state_path, save_state=False):\n",
        "    if batch > 0 and batch % (500 // batch_size) == 0:\n",
        "      \n",
        "      last_seq_path = \"/content/drive/My Drive/bert_last_i.txt\"\n",
        "      with open(last_seq_path, 'w+') as seq_record:\n",
        "        seq_record.write(str(batch))\n",
        "      loss_trace_path = \"/content/drive/My Drive/bert_loss_trace.txt\"\n",
        "      with open(loss_trace_path, 'a+') as loss_record:\n",
        "        loss_record.write(str(batch) + ' loss: ' + str(total_loss/100) + '\\n')\n",
        "      if save_state:\n",
        "        torch.save(model.state_dict(), state_path)\n",
        "    if batch > 0 and batch % (100 // batch_size) == 0:\n",
        "      print(\"Batch: \" + str(batch) + \" loss: \" + str(total_loss / 100))\n",
        "      total_loss = 0\n",
        "    return total_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2Ijzh8ZLnjwN",
        "outputId": "39e4d3d3-f529-40a5-ce56-e339fca225ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "batch_size = 1\n",
        "MAX_LEN = 1002\n",
        "FOURMERS = 'fourmers'\n",
        "HEXAMERS = 'hexamers'\n",
        "OCTAMERS = 'octamers'\n",
        "\n",
        "tokenizer, formatted_hexamers, attention_masks, labels = process_data('/content/drive/My Drive/partitioned_samples/split_samples_8-mer/split_samples_8-mer_train.txt', OCTAMERS)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "torch.cuda.get_device_name(0) \n",
        "\n",
        "model = BertForTokenClassification(BertConfig.from_json_file('/content/drive/My Drive/bert_configuration.json'))\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "model.to(device)\n",
        "\n",
        "#model.load_state_dict(torch.load(\"/content/drive/My Drive/bert_splice_weights_2.pt\"))\n",
        "last_i = 0\n",
        "with open('/content/drive/My Drive/bert_last_i.txt', 'r') as last_i_file:\n",
        "  i = last_i_file.read()\n",
        "  last_i = int(i)\n",
        "\n",
        "print(last_i)\n",
        "optimizer = Adam(model.parameters(), lr=3e-5)\n",
        "\n",
        "training_loss = []\n",
        "model.train()\n",
        "class_weights = torch.tensor(np.array([1.0, 165.0])).float().cuda()\n",
        "loss = CrossEntropyLoss(weight=class_weights)\n",
        "print(\"ready to train\")\n",
        "np.random.shuffle(formatted_hexamers)\n",
        "total_loss = 0\n",
        "predictions_array = []\n",
        "labels_array = []\n",
        "for batch in range(len(formatted_hexamers) // batch_size):\n",
        "    seq_batch = formatted_hexamers[batch*batch_size:(batch+1)*batch_size]\n",
        "    attention_masks_batch = attention_masks[batch*batch_size:(batch+1)*batch_size]\n",
        "    attention_masks_batch = torch.tensor(attention_masks_batch).long().cuda()\n",
        "    split_seq_batch = seq_batch[0].split()\n",
        "    batch_ids = [tokenizer.convert_tokens_to_ids(split_seq_batch)]\n",
        "    batch_ids = pad_sequences(batch_ids, maxlen=MAX_LEN, truncating='post', padding='post')[0]\n",
        "    batch_ids = torch.tensor(batch_ids).unsqueeze(0).long().cuda()\n",
        "    batch_labs = labels[batch*batch_size:(batch+1)*batch_size]\n",
        "    flat_labs = list(itertools.chain(*batch_labs))\n",
        "    labels_array.extend(flat_labs)\n",
        "    #print(np.shape(labels_array))\n",
        "    batch_labels = torch.tensor(batch_labs).long().cuda()\n",
        "\n",
        "    \n",
        "    predictions = model(batch_ids, attention_mask=attention_masks_batch)\n",
        "    softmax_preds = softmax(predictions[0], dim=2).cpu().detach().numpy()\n",
        "    flat_softmax = np.array(list(itertools.chain(*softmax_preds)))\n",
        "    predictions_array.extend(list(flat_softmax[:,-1:]))\n",
        "    #print(np.shape(predictions_array))\n",
        "    l = loss(predictions[0].squeeze(0), batch_labels[0])\n",
        "    total_loss += l.item()\n",
        "    training_loss.append(l.item())\n",
        "    \n",
        "    gradient_descent(l)\n",
        "    state_path = \"/content/drive/My Drive/bert_splice_weights/octamers_model_weights.pt\"\n",
        "    total_loss = write_data(batch, batch_size, labels_array, total_loss, state_path, save_state = True)\n",
        "\n",
        "#plot_AUCPRC(labels_array, predictions_array)\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3000\n",
            "ready to train\n",
            "Batch: 100 loss: 0.8090678329579533\n",
            "Batch: 200 loss: 0.7973055811226368\n",
            "Batch: 300 loss: 0.6902930942177773\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}